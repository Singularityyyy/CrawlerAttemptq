

Originally, the whole idea of building this project had N consecutive points:
1. Set an entry point (a url) which would help me grab other urls that the "entry" url's page contained (can be processed DEEPLY, replacing the so-called "entry point" with all the urls that the "entry point" had).
2. Form a massive list of data.
3. Fullful a .db file with at least several hundred thousand rows of records.
-3.5- repeat 1-3 until a satisfying number of websites are included into the database.
4. Start the first iteration of the system (filling columns of "date" and "status").
5. Set the key amount of days that may pass without having to update the HTTP status.
6. Run the application, update'm statutes'n'dates (if needed).

Having done step 3, I realized I was able to retrieve-extract-reformat and import the first 1 million of world's most visited/popular websites (Alexa gave out coupies of a spcial .csv file about 5 years ago) into the DB.


